This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
ao-cli/
  src/
    opts/
      language.rs
      mod.rs
    build.rs
    check.rs
    config.rs
    init.rs
    lib.rs
    main.rs
    run.rs
    utils.rs
  .gitignore
  Cargo.toml
api-service/
  tests/
    test_main.py
  .pytest.ini
  .python-version
  anops_pb2_grpc.py
  anops_pb2.py
  anops_pb2.pyi
  Dockerfile
  main.py
  pyproject.toml
  README.md
  requirements-dev.txt
  requirements.txt
model-interface/
  anops.proto
  README.md
model-service/
  tests/
    test_server.py
  anops_pb2_grpc.py
  anops_pb2.py
  anops_pb2.pyi
  Dockerfile
  README.md
  requirements-dev.txt
  requirements.txt
  server.py
tests/
  test_e2e.py
.pre-commit-config.yaml
.repomixignore
ao.toml
before-production.md
CONTRIBUTING.md
docker-compose.yml
README.md
repomix.config.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="ao-cli/src/opts/language.rs">
pub enum Language {
</file>

<file path="ao-cli/src/opts/mod.rs">
pub mod language;
</file>

<file path="ao-cli/src/lib.rs">
pub mod init;
pub mod check;
pub mod config;
pub mod run;
pub mod utils;
pub mod build; // Add the build module
</file>

<file path="ao-cli/.gitignore">
/target
entire-codebase.xml
repomix.config.json
.repomixignore
</file>

<file path="api-service/.pytest.ini">
[pytest]
addopts =
    --cov-report=xml
    --cov-report=html
</file>

<file path="api-service/.python-version">
3.12
</file>

<file path="api-service/anops_pb2_grpc.py">
# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
⋮----
GRPC_GENERATED_VERSION = '1.71.0'
GRPC_VERSION = grpc.__version__
_version_not_supported = False
⋮----
_version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
⋮----
_version_not_supported = True
⋮----
class AnOpsStub(object)
⋮----
"""The AnOps service definition.
    """
⋮----
def __init__(self, channel)
⋮----
"""Constructor.

        Args:
            channel: A grpc.Channel.
        """
⋮----
class AnOpsServicer(object)
⋮----
def Predict(self, request, context)
⋮----
"""Sends input data for prediction.
        """
⋮----
def add_AnOpsServicer_to_server(servicer, server)
⋮----
rpc_method_handlers = {
generic_handler = grpc.method_handlers_generic_handler(
⋮----
# This class is part of an EXPERIMENTAL API.
class AnOps(object)
</file>

<file path="api-service/anops_pb2.py">
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: anops.proto
# Protobuf Python Version: 5.29.0
"""Generated protocol buffer code."""
⋮----
# @@protoc_insertion_point(imports)
⋮----
_sym_db = _symbol_database.Default()
⋮----
DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0b\x61nops.proto\x12\x05\x61nops\"$\n\x0ePredictRequest\x12\x12\n\ninput_data\x18\x01 \x01(\t\"&\n\x0fPredictResponse\x12\x13\n\x0boutput_data\x18\x01 \x01(\t2C\n\x05\x41nOps\x12:\n\x07Predict\x12\x15.anops.PredictRequest\x1a\x16.anops.PredictResponse\"\x00\x62\x06proto3')
⋮----
_globals = globals()
⋮----
# @@protoc_insertion_point(module_scope)
</file>

<file path="api-service/anops_pb2.pyi">
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from typing import ClassVar as _ClassVar, Optional as _Optional

DESCRIPTOR: _descriptor.FileDescriptor

class PredictRequest(_message.Message):
    __slots__ = ("input_data",)
    INPUT_DATA_FIELD_NUMBER: _ClassVar[int]
    input_data: str
    def __init__(self, input_data: _Optional[str] = ...) -> None: ...

class PredictResponse(_message.Message):
    __slots__ = ("output_data",)
    OUTPUT_DATA_FIELD_NUMBER: _ClassVar[int]
    output_data: str
    def __init__(self, output_data: _Optional[str] = ...) -> None: ...
</file>

<file path="api-service/pyproject.toml">
[project]
name = "api-service"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "fastapi>=0.115.12",
    "grpcio>=1.71.0",
    "python-json-logger>=3.3.0",
    "uvicorn[standard]>=0.34.2",
]

[dependency-groups]
dev = [
    "pytest>=8.3.5",
    "pytest-cov>=6.1.1",
    "ruff>=0.11.8",
]
</file>

<file path="api-service/README.md">
# AnOps API Service

## Overview
The AnOps API Service is a RESTful API that provides access to the underlying `model-service` via a simple HTTP interface. It acts as a bridge between standard web requests and the gRPC-based `model-service`.

**Technology Choice:** This service is implemented using **Python** and the **FastAPI** framework. This choice was made for:
*   **Simplicity and Speed:** FastAPI allows for rapid development of robust APIs.
*   **Python Ecosystem:** Leverages the rich Python ecosystem, familiar to the target audience.
*   **Async Support:** Good support for asynchronous operations, beneficial for handling I/O like gRPC calls.
*   **Automatic Docs:** Built-in OpenAPI (Swagger) documentation generation.

## Functionality
*   Receives requests (e.g., JSON payloads) on defined REST endpoints (e.g., `/predict`).
*   Connects to the `model-service` using gRPC (acting as a gRPC client).
*   Forwards the request data to the `model-service` according to the `model-interface` definition.
*   Receives the response from the `model-service`.
*   Formats and returns the response to the original HTTP caller (e.g., as JSON).

## Running the Service
This service is designed to be run as a Docker container. See the `Dockerfile` and the root `docker-compose.yml`.
</file>

<file path="api-service/requirements-dev.txt">
pytest>=7.0.0,<8.0.0
pytest-cov>=4.0.0,<5.0.0
httpx>=0.24.0,<1.0.0
</file>

<file path="model-interface/anops.proto">
syntax = "proto3";

package anops;

// The AnOps service definition.
service AnOps {
  // Sends input data for prediction.
  rpc Predict (PredictRequest) returns (PredictResponse) {}
}

// The request message containing the input data.
// For simplicity, starting with a generic string input.
// This will likely evolve to support structured data (e.g., JSON string, bytes, etc.).
message PredictRequest {
  string input_data = 1;
}

// The response message containing the prediction result.
// Similarly generic for now.
message PredictResponse {
  string output_data = 1;
}
</file>

<file path="model-service/anops_pb2_grpc.py">
# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
⋮----
GRPC_GENERATED_VERSION = '1.71.0'
GRPC_VERSION = grpc.__version__
_version_not_supported = False
⋮----
_version_not_supported = first_version_is_lower(GRPC_VERSION, GRPC_GENERATED_VERSION)
⋮----
_version_not_supported = True
⋮----
class AnOpsStub(object)
⋮----
"""The AnOps service definition.
    """
⋮----
def __init__(self, channel)
⋮----
"""Constructor.

        Args:
            channel: A grpc.Channel.
        """
⋮----
class AnOpsServicer(object)
⋮----
def Predict(self, request, context)
⋮----
"""Sends input data for prediction.
        """
⋮----
def add_AnOpsServicer_to_server(servicer, server)
⋮----
rpc_method_handlers = {
generic_handler = grpc.method_handlers_generic_handler(
⋮----
# This class is part of an EXPERIMENTAL API.
class AnOps(object)
</file>

<file path="model-service/anops_pb2.py">
# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# NO CHECKED-IN PROTOBUF GENCODE
# source: anops.proto
# Protobuf Python Version: 5.29.0
"""Generated protocol buffer code."""
⋮----
# @@protoc_insertion_point(imports)
⋮----
_sym_db = _symbol_database.Default()
⋮----
DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0b\x61nops.proto\x12\x05\x61nops\"$\n\x0ePredictRequest\x12\x12\n\ninput_data\x18\x01 \x01(\t\"&\n\x0fPredictResponse\x12\x13\n\x0boutput_data\x18\x01 \x01(\t2C\n\x05\x41nOps\x12:\n\x07Predict\x12\x15.anops.PredictRequest\x1a\x16.anops.PredictResponse\"\x00\x62\x06proto3')
⋮----
_globals = globals()
⋮----
# @@protoc_insertion_point(module_scope)
</file>

<file path="model-service/anops_pb2.pyi">
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from typing import ClassVar as _ClassVar, Optional as _Optional

DESCRIPTOR: _descriptor.FileDescriptor

class PredictRequest(_message.Message):
    __slots__ = ("input_data",)
    INPUT_DATA_FIELD_NUMBER: _ClassVar[int]
    input_data: str
    def __init__(self, input_data: _Optional[str] = ...) -> None: ...

class PredictResponse(_message.Message):
    __slots__ = ("output_data",)
    OUTPUT_DATA_FIELD_NUMBER: _ClassVar[int]
    output_data: str
    def __init__(self, output_data: _Optional[str] = ...) -> None: ...
</file>

<file path="model-service/README.md">
# AnOps Model Service

## Overview
The AnOps Model Service is responsible for hosting and executing the actual machine learning or statistical model. It exposes a gRPC interface defined in `../model-interface` and listens for requests from the `api-service`.

**Technology Choice:** This service is primarily intended to be implemented in **Python**, leveraging common data science libraries (e.g., scikit-learn, statsmodels, pandas, numpy) and the `grpcio` library for the gRPC server. Support for R (e.g., using Plumber with a gRPC bridge) is a future consideration.

## Functionality
*   Implements the gRPC server interface defined by `../model-interface/anops.proto`.
*   Loads the user-provided model code and any necessary artifacts (e.g., trained model files).
*   Receives prediction requests from the `api-service` via gRPC.
*   Processes the input data using the loaded model.
*   Returns the prediction results back to the `api-service` via gRPC.

## Running the Service
This service is designed to be run as a Docker container. See the `Dockerfile` and the root `docker-compose.yml`. The user's model code and dependencies will be packaged into this container image during the `ao build` process.
</file>

<file path="model-service/requirements-dev.txt">
pytest>=7.0.0,<8.0.0
pytest-cov>=4.0.0,<5.0.0
grpcio-testing>=1.50.0,<2.0.0
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/psf/black
    rev: 24.3.0
    hooks:
      - id: black
        language_version: python3
  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.3.0
    hooks:
      - id: ruff
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: end-of-file-fixer
      - id: trailing-whitespace
</file>

<file path=".repomixignore">
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/

ACTIONPLAN.md
tmp/
Cargo.lock
target/
</file>

<file path="CONTRIBUTING.md">
# Contributing to AnOps

Thank you for your interest in contributing to AnOps!

## How to Contribute

1. **Fork the repository** and create a new branch for your feature or bugfix.
2. **Write clear, well-documented code** that follows the style of the project.
3. **Add or update tests** for your changes. All code must be covered by unit, integration, or end-to-end tests.
4. **Run `ao check`** to ensure all tests and linters pass before submitting a pull request.
5. **Document your changes** in the relevant README or code comments if needed.
6. **Submit a pull request** with a clear description of your changes and why they are needed.

## Code Style
- Python: Use [black](https://black.readthedocs.io/en/stable/) for formatting and [ruff](https://beta.ruff.rs/docs/) for linting.
- Rust: Use `cargo fmt` and `cargo clippy`.
- All code must be tested and pass CI before merging.

## Project Structure
- `ao-cli/`: Rust CLI tool
- `api-service/`: FastAPI REST API
- `model-service/`: Python gRPC model server
- `model-interface/`: gRPC proto definitions
- `tests/`: End-to-end tests

## Reporting Issues
- Please use GitHub Issues to report bugs or request features.

## Code of Conduct
- Be respectful and constructive. See [Contributor Covenant](https://www.contributor-covenant.org/) for guidance.
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  api-service:
    build: ./api-service
    ports:
      - "8000:8000" # Expose API port
    environment:
      # Example: Point API to the gRPC service
      MODEL_SERVICE_URL: model-service:50051
    depends_on:
      - model-service
    networks:
      - anops-net

  model-service:
    build: ./model-service
    ports:
      - "50051:50051" # Expose gRPC port (optional for external access, needed for API)
    networks:
      - anops-net

networks:
  anops-net:
    driver: bridge
</file>

<file path="repomix.config.json">
{
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "entire-codebase.xml",
    "style": "xml",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

<file path="ao-cli/src/main.rs">
use anyhow::Result;
⋮----
use std::fs::OpenOptions;
⋮----
use ao::{init, check, run, build}; // Added build
⋮----
/// Top-level CLI parser
⋮----
struct Cli {
⋮----
/// Available subcommands
⋮----
enum Commands {
/// Initialize a new modeling project
⋮----
/// Name of the project to initialize
⋮----
/// Run linting and tests on a project
⋮----
/// Path to the project directory
⋮----
/// Run a defined task from ao.toml
⋮----
/// Name of the task to run
⋮----
/// Path within the project directory (optional, defaults to current dir)
⋮----
/// Build Docker images for the project services
⋮----
fn main() -> Result<()> {
// Initialize JSON logger to file
⋮----
.create(true)
.append(true)
.open("ao-cli.log")
.expect("Failed to open log file");
⋮----
.with_writer(log_file)
.json()
.with_env_filter(EnvFilter::from_default_env())
.init();
⋮----
Ok(())
</file>

<file path="api-service/tests/test_main.py">
client = TestClient(main.app)
⋮----
def test_health_check()
⋮----
response = client.get("/health")
⋮----
@pytest.mark.asyncio
@patch("main.anops_pb2_grpc.AnOpsStub")
async def test_predict_success(mock_stub)
⋮----
# Mock the gRPC stub's Predict method
mock_predict = AsyncMock()
⋮----
response = await client.post("/predict", json={"input_data": "test input"})
⋮----
@pytest.mark.asyncio
@patch("main.anops_pb2_grpc.AnOpsStub")
async def test_predict_invalid_argument(mock_stub)
⋮----
# Simulate gRPC INVALID_ARGUMENT error
⋮----
error = main.grpc.aio.AioRpcError(
⋮----
response = await client.post("/predict", json={"input_data": ""})
⋮----
@pytest.mark.asyncio
@patch("main.anops_pb2_grpc.AnOpsStub")
async def test_predict_service_unavailable(mock_stub)
⋮----
# Simulate gRPC UNAVAILABLE error
⋮----
error = main.grpc.aio.AioRpcError(main.grpc.StatusCode.UNAVAILABLE, "Service down")
⋮----
response = await client.post("/predict", json={"input_data": "test"})
⋮----
@pytest.mark.asyncio
@patch("main.anops_pb2_grpc.AnOpsStub")
async def test_predict_internal_error(mock_stub)
⋮----
# Simulate generic gRPC error
⋮----
error = main.grpc.aio.AioRpcError(main.grpc.StatusCode.INTERNAL, "Internal error")
⋮----
@pytest.mark.asyncio
@patch("main.anops_pb2_grpc.AnOpsStub")
async def test_predict_unexpected_exception(mock_stub)
⋮----
# Simulate unexpected exception
⋮----
def test_predict_malformed_request()
⋮----
# Missing required field 'input_data'
response = client.post("/predict", json={})
⋮----
def test_predict_env_var(monkeypatch)
⋮----
# Patch the environment variable and reload the app
⋮----
# The app should pick up the env var (this is a basic check, deeper checks require more refactoring)
</file>

<file path="api-service/Dockerfile">
# Use an official Python runtime as a parent image
FROM python:3.11-slim

# Create a non-root user
RUN useradd -m appuser

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy only necessary files
COPY main.py .
COPY anops_pb2.py .
COPY anops_pb2_grpc.py .
COPY anops_pb2.pyi .

# Switch to non-root user
USER appuser

# Expose the port the app runs on
EXPOSE 8000

# Command to run the application using Uvicorn
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
</file>

<file path="model-interface/README.md">
# AnOps Model Interface (gRPC)

This directory contains the Protocol Buffer (`.proto`) definitions for the gRPC interface used for communication between the `api-service` and the `model-service`.

## Overview

The `api-service` acts as a gRPC client, sending requests to the `model-service`, which acts as a gRPC server implementing the defined service.

## Service Definition

See `anops.proto` for the formal service and message definitions.

## Generating Code Stubs

The necessary Python client and server code stubs (`*_pb2.py`, `*_pb2_grpc.py`, `*_pb2.pyi`) are **automatically generated** when you run the `ao build` command.

The generated files are placed into both the `api-service` and `model-service` directories. You do not need to run `protoc` manually. Ensure you have `grpcio-tools` installed in the Python environment where you run `ao build` (or ensure `python -m grpc_tools.protoc` is runnable).
</file>

<file path="model-service/tests/test_server.py">
# Import the modules to be tested
⋮----
# --- Unit Tests for Model Logic --- #
⋮----
def test_load_model_resource_default()
⋮----
"""Test loading the default prefix."""
# Unset env var if it exists to test default
⋮----
prefix = server.load_model_resource()
⋮----
def test_load_model_resource_env_var()
⋮----
"""Test loading the prefix from environment variable."""
test_prefix = "TEST_PREFIX:"
⋮----
del os.environ["MODEL_PREFIX"]  # Clean up
⋮----
def test_load_model_resource_empty_env()
⋮----
"""Test loading the prefix when environment variable is empty."""
⋮----
def test_run_model_success()
⋮----
"""Test successful model execution."""
# Ensure default prefix is used for consistency
⋮----
server.MODEL_PREFIX = server.load_model_resource()  # Reload prefix
⋮----
input_data = "hello world"
expected_output = "MODEL_OUTPUT: HELLO WORLD"
output = server.run_model(input_data)
⋮----
def test_run_model_empty_input()
⋮----
"""Test model execution with empty input."""
⋮----
# --- Integration Tests for gRPC Servicer --- #
⋮----
# Fixture to set up and tear down the gRPC server for testing
⋮----
@pytest.fixture(scope="module")
def grpc_server()
⋮----
test_server = grpc.server(futures.ThreadPoolExecutor(max_workers=1))
⋮----
port = test_server.add_insecure_port("[::]:0")  # Use random available port
⋮----
yield f"localhost:{port}"  # Provide the server address to tests
⋮----
def test_predict_success(grpc_server)
⋮----
"""Test the Predict RPC call with valid input."""
⋮----
stub = anops_pb2_grpc.AnOpsStub(channel)
request = anops_pb2.PredictRequest(input_data="test input")
response = stub.Predict(request)
⋮----
# Assuming default prefix MODEL_OUTPUT:
⋮----
def test_predict_invalid_argument(grpc_server)
⋮----
"""Test the Predict RPC call with invalid (empty) input."""
⋮----
request = anops_pb2.PredictRequest(input_data="")
⋮----
def test_predict_internal_error(grpc_server)
⋮----
"""Test the Predict RPC call with internal server error."""
# Patch run_model to raise a generic Exception
⋮----
# TODO: Add test for internal server error (e.g., by mocking run_model to raise Exception)
</file>

<file path="model-service/Dockerfile">
# Use an official Python runtime as a parent image
FROM python:3.11-slim

# Create a non-root user
RUN useradd -m appuser

# Set the working directory in the container
WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy only necessary files
COPY server.py .
COPY anops_pb2.py .
COPY anops_pb2_grpc.py .
COPY anops_pb2.pyi .

# Switch to non-root user
USER appuser

# Expose the port the gRPC server listens on
EXPOSE 50051

# Command to run the gRPC server
CMD ["python", "server.py"]
</file>

<file path="tests/test_e2e.py">
API_URL = "http://localhost:8000/predict"
HEALTH_URL = "http://localhost:8000/health"
⋮----
def wait_for_health(timeout=60)
⋮----
start = time.time()
⋮----
resp = requests.get(HEALTH_URL, timeout=2)
⋮----
def main()
⋮----
up = subprocess.run(["docker-compose", "up", "--build", "-d"], capture_output=True)
⋮----
resp = requests.post(API_URL, json={"input_data": "hello world"}, timeout=5)
⋮----
data = resp.json()
⋮----
def test_api_when_model_service_down()
⋮----
# Start only api-service, do not start model-service
up = subprocess.run(
⋮----
resp = requests.post(API_URL, json={"input_data": "test"}, timeout=5)
</file>

<file path="ao.toml">
[project]
name = "anops-dev"

[check]
linters = []
testers = [
    "pytest --cov=server.py --cov-report=term --maxfail=1 --disable-warnings -C model-service/tests",
    "pytest --cov=main.py --cov-report=term --maxfail=1 --disable-warnings -C api-service/tests",
    "pytest --maxfail=1 --disable-warnings -C tests"
]

# [tasks]
# my-task = ["echo hello"]
</file>

<file path="before-production.md">
# Before Production Checklist for AnOps

This checklist is for the final pre-production review of the AnOps project. It is based on a critical review of the codebase and aims to prevent common deployment failures and technical debt. All items must be checked off before the first production release.

## 1. Test Coverage
- [x] **Rust CLI (`ao-cli`)**
  - [x] Integration tests for `ao build` and `ao check` (verify Docker images and gRPC code are actually built)
  - [x] Mock external commands in tests (e.g., using `assert_cmd` or similar)
- [x] **Python Services**
  - [x] `api-service`: Tests for startup/shutdown, environment variable handling, and malformed requests
  - [x] `model-service`: Test for internal server error in gRPC, and for environment misconfiguration
- [x] **End-to-End**
  - [x] E2E tests: Add teardown/cleanup logic
  - [x] E2E tests: Add negative-path tests (e.g., model-service down, invalid input)

## 2. Coding Best Practices
- [ ] **Logging**
  - [ ] Use structured logging in Rust (`log` crate)
  - [ ] Set log levels and formats in Python services
- [ ] **Error Handling**
  - [ ] Ensure all user-facing errors are actionable and clear
- [ ] **Security**
  - [ ] Review Dockerfiles for unnecessary packages/files

## 3. Project Organization
- [ ] **Documentation**
  - [ ] Ensure `ACTIONPLAN.md` is up to date and reflects the current state
  - [ ] Add developer onboarding section to `README.md` (how to run tests, what to expect)

## 4. Automation
- [ ] **CI/CD**
  - [ ] Ensure all tests (unit, integration, E2E) are run in CI
  - [ ] Ensure CI failures are visible and actionable

## 5. Outstanding TODOs in Code
- [ ] `ao-cli/src/build.rs`: Implement tests for build logic
- [ ] `ao-cli/src/utils.rs`: Add robust tests for `run_tool` and gRPC code generation
- [ ] `model-service/server.py`: Add test for internal server error
- [ ] `tests/test_e2e.py`: Add teardown and negative-path tests

---

**Sign-off:** All items above must be checked and signed off by the responsible engineer before production deployment.
</file>

<file path="README.md">
# AnOps

AnOps is an orchestration tool for deploying and managing machine learning models in production. It is designed to simplify the process of deploying models by providing a consistent interface for all models, regardless of their underlying technology or implementation. AnOps is built on top of Docker and gRPC, making it easy to deploy and manage models in any environment that supports these technologies.

## Components

- **ao CLI**: Command-line tool for project management, validation, and automation.
- **api-service**: REST API (FastAPI) that exposes model predictions via HTTP and forwards requests to model-service via gRPC.
- **model-service**: Python (or R, future) service that implements the gRPC interface and runs the actual model logic.
- **model-interface**: Protocol Buffers definition for the gRPC contract between api-service and model-service.

## Quickstart

1. **Initialize a new project**
   ```sh
   ao init myproject
   cd myproject
   ```
2. **Implement your model** in `model-service/server.py` and update the gRPC interface in `model-interface/anops.proto` if needed.
3. **Build and generate code**
   ```sh
   ao build
   ```
   This will generate gRPC code and build Docker images for both services.
4. **Run the stack**
   ```sh
   docker-compose up --build
   ```
5. **Test the API**
   ```sh
   curl -X POST http://localhost:8000/predict -H 'Content-Type: application/json' -d '{"input_data": "hello world"}'
   ```

## Testing

- Run all tests (unit, integration, end-to-end):
  ```sh
  ao check
  ```
- Tests are located in `api-service/tests/`, `model-service/tests/`, and `tests/` (end-to-end).

## Best Practices

- All code is tested and linted before build and deploy.
- Docker images run as non-root users and only include necessary files.
- gRPC code generation is automated via `ao build`.
- Project structure and configuration are validated with `ao check`.

## Contributing

See `ACTIONPLAN.md` for the current development roadmap and checklist.

## License

MIT
</file>

<file path="ao-cli/src/build.rs">
use std::path::Path;
⋮----
use crate::config;
use crate::utils::{find_project_root, run_tool, generate_grpc_code}; // Added generate_grpc_code
use crate::check; // Import the check module to run pre-build checks
⋮----
/// Handler for `ao build`.
/// Finds the project root, loads config, generates gRPC code, runs checks, and builds Docker images.
///
/// # Arguments
⋮----
/// * `path_str` - Path within the project directory to start searching from.
⋮----
/// # Errors
⋮----
/// Returns an error if the project root is not found, config loading fails,
/// gRPC generation fails, checks fail, or any Docker build command fails.
pub fn run(path_str: String) -> Result<()> {
⋮----
info!("Starting build from {}", start_path.display());
⋮----
// Find project root
let project_path = find_project_root(start_path)
.with_context(|| format!("Failed to find project root starting from '{}'", start_path.display()))?;
info!("Found project root at {}", project_path.display());
⋮----
// Load configuration
⋮----
.context("Failed to load project configuration")?;
⋮----
info!("Building project: {}", project_name);
⋮----
// --- Generate gRPC Code --- //
generate_grpc_code(&project_path)
.context("Failed to generate gRPC code")?;
// --- End Generate gRPC Code --- //
⋮----
// --- Pre-build Checks --- //
info!("--- Running Pre-Build Checks ---");
// Use the existing check::run function
check::run(path_str.clone()) // Pass the original path string
.context("Pre-build checks failed")?;
info!("--- Pre-Build Checks Passed ---");
⋮----
// --- Build Docker Images --- //
info!("--- Building Docker Images ---");
⋮----
// Define image names (using project name from config)
// TODO: Allow overriding tags/names via config or CLI args later
let api_image_name = format!("{}-api-service:latest", project_name);
let model_image_name = format!("{}-model-service:latest", project_name);
⋮----
// Build api-service
let api_service_path = project_path.join("api-service");
if api_service_path.exists() && api_service_path.is_dir() {
info!("Building {}...", api_image_name);
let build_cmd = format!(
⋮----
run_tool(&build_cmd, &api_service_path)
.with_context(|| format!("Failed to build api-service image: {}", api_image_name))?;
info!("Successfully built {}", api_image_name);
⋮----
warn!("Skipping api-service build: directory not found at {:?}", api_service_path);
⋮----
// Build model-service
let model_service_path = project_path.join("model-service");
if model_service_path.exists() && model_service_path.is_dir() {
info!("Building {}...", model_image_name);
// Note: This assumes the Docker context is the model-service directory itself.
// If generated gRPC code needs to be included from model-interface,
// the Dockerfile or build process might need adjustment (e.g., copying files before build).
⋮----
run_tool(&build_cmd, &model_service_path)
.with_context(|| format!("Failed to build model-service image: {}", model_image_name))?;
info!("Successfully built {}", model_image_name);
⋮----
warn!("Skipping model-service build: directory not found at {:?}", model_service_path);
⋮----
info!("--- Docker Images Built Successfully ---");
⋮----
Ok(())
⋮----
mod tests {
⋮----
use crate::init;
use tempfile::tempdir;
use std::fs;
use std::path::PathBuf;
⋮----
// Helper to create a valid project structure for testing build
fn setup_valid_project(base_path: &std::path::Path) -> PathBuf {
⋮----
let project_path = base_path.join(project_name);
init::run(project_path.to_str().unwrap().to_string()).unwrap();
⋮----
// Basic integration test for build::run
// This test will not actually build Docker images, but will check that the logic flows and errors are handled.
⋮----
fn build_run_succeeds_with_valid_project() {
let tmp_dir = tempdir().unwrap();
let project_path = setup_valid_project(tmp_dir.path());
// Overwrite Dockerfiles with a minimal valid Dockerfile to avoid build errors
⋮----
fs::write(project_path.join("api-service/Dockerfile"), dockerfile).unwrap();
fs::write(project_path.join("model-service/Dockerfile"), dockerfile).unwrap();
// Overwrite ao.toml with minimal config
fs::write(project_path.join("ao.toml"), "[project]\nname = 'test_build_project'").unwrap();
// This will likely fail at the gRPC codegen or docker build step if dependencies are missing,
// but we want to ensure it does not panic and returns an error with context.
let result = run(project_path.to_str().unwrap().to_string());
⋮----
Ok(_) => info!("build::run returned Ok (all dependencies found)"),
⋮----
let msg = e.to_string();
warn!("build::run returned Err: {}", msg);
// Acceptable errors: gRPC codegen or docker build failures
assert!(msg.contains("Failed to generate gRPC code") ||
</file>

<file path="ao-cli/src/check.rs">
use std::fs;
⋮----
use crate::config; // Import the config module
use crate::utils::{find_project_root, run_tool}; // Import from utils
⋮----
/// Handler for `ao check`.
/// Verifies structure, loads config, and runs configured linters/testers.
///
/// # Arguments
⋮----
/// * `path_str` - Path within the project directory to start searching from.
⋮----
/// # Errors
⋮----
/// Returns an error if any step (root finding, config load, structure check, tool execution) fails.
pub fn run(path_str: String) -> Result<()> {
⋮----
info!("Starting check from {}", start_path.display());
⋮----
// Find project root
let project_path = find_project_root(start_path)
.with_context(|| format!("Failed to find project root starting from '{}'", start_path.display()))?;
info!("Found project root at {}", project_path.display());
⋮----
// Load configuration
⋮----
.context("Failed to load project configuration")?;
info!("Project name from config: {}", config.project.name);
⋮----
// --- Structure Checks --- //
info!("Running structure checks in '{}'", project_path.display());
⋮----
// Check for required directories relative to the found project root
// TODO: Enhance check to include service directories // Updated below
⋮----
// Keep original basic checks? Decide if they are still relevant
// "models",
// "tests",
// "notebooks",
⋮----
for dir_name in required_dirs.iter() {
let dir_path = project_path.join(dir_name);
if !dir_path.exists() {
bail!(
⋮----
if !dir_path.is_dir() {
bail!("Path '{}' is not a directory.", dir_path.display());
⋮----
info!("Found directory: {:?}", dir_path);
⋮----
// Check for specific required files within service directories
⋮----
("api-service", "requirements.txt"), // Good practice to check
("api-service", "anops_pb2.py"), // Check for generated gRPC file
("api-service", "anops_pb2_grpc.py"), // Check for generated gRPC file
⋮----
("model-service", "requirements.txt"), // Good practice to check
("model-service", "anops_pb2.py"), // Check for generated gRPC file
("model-service", "anops_pb2_grpc.py"), // Check for generated gRPC file
⋮----
for (dir_name, file_name) in required_files.iter() {
let file_path = project_path.join(dir_name).join(file_name);
if !file_path.exists() {
⋮----
if !file_path.is_file() {
bail!("Path '{}' is not a file.", file_path.display());
⋮----
info!("Found file: {:?}", file_path);
⋮----
// Config file presence is already checked by find_project_root and load_config
info!("Found config file: {:?}", project_path.join("ao.toml"));
⋮----
// --- Tool Execution --- //
⋮----
// Run configured linters
if (!config.check.linters.is_empty()) {
info!("--- Running Linters ---");
⋮----
run_tool(linter_cmd, &project_path)
.with_context(|| format!("Linter command '{}' failed", linter_cmd))?;
⋮----
info!("--- Linters Finished ---");
⋮----
info!("No linters configured.");
⋮----
// Run configured testers
if (!config.check.testers.is_empty()) {
info!("--- Running Testers ---");
⋮----
run_tool(tester_cmd, &project_path)
.with_context(|| format!("Tester command '{}' failed", tester_cmd))?;
⋮----
info!("--- Testers Finished ---");
⋮----
info!("No testers configured.");
⋮----
info!("All checks passed successfully!");
Ok(())
⋮----
mod tests {
⋮----
use crate::init;
⋮----
use tempfile::tempdir;
⋮----
// Helper to create a valid project structure for testing check
// Note: init::run already creates the required service structure
fn setup_valid_project(base_path: &Path) -> Result<PathBuf> {
⋮----
let project_path = base_path.join(project_name);
// Use init::run to create the structure
init::run(project_path.to_str().unwrap().to_string())
.context("init::run failed during test setup")?;
Ok(project_path)
⋮----
// Helper to add a [check] section to ao.toml
fn add_check_config(project_path: &Path) {
let config_path = project_path.join("ao.toml");
let mut content = fs::read_to_string(&config_path).unwrap();
content.push_str(
⋮----
fs::write(config_path, content).unwrap();
⋮----
fn find_project_root_finds_root() {
let tmp_dir = tempdir().unwrap();
let project_path = setup_valid_project(tmp_dir.path()).unwrap();
// Ensure 'models' directory exists for the test
let models_path = project_path.join("models");
std::fs::create_dir_all(&models_path).unwrap();
⋮----
// Search from root
let found_root = crate::utils::find_project_root(&project_path).unwrap();
// Compare canonicalized paths for robustness
let expected = project_path.canonicalize().unwrap();
let actual = found_root.canonicalize().unwrap();
assert_eq!(expected, actual);
⋮----
// Search from subdir
let found_root_from_subdir = crate::utils::find_project_root(&models_path).unwrap();
let actual_subdir = found_root_from_subdir.canonicalize().unwrap();
assert_eq!(expected, actual_subdir);
⋮----
fn find_project_root_fails_if_no_root() {
⋮----
let project_path = tmp_dir.path().join("no_config_project");
fs::create_dir(&project_path).unwrap();
⋮----
let result = crate::utils::find_project_root(&project_path); // Use utils::find_project_root
assert!(result.is_err());
assert!(result
⋮----
fn run_succeeds_when_called_from_root() {
⋮----
let result = run(project_path.to_str().unwrap().to_string());
assert!(result.is_ok());
⋮----
fn run_succeeds_when_called_from_subdir() {
⋮----
let models_path = project_path.join("models"); // 'models' dir is created by setup_valid_project via init::run
let result = run(models_path.to_str().unwrap().to_string());
⋮----
fn run_succeeds_with_check_config_present() {
⋮----
add_check_config(&project_path); // Add [check] section
⋮----
// We could capture stdout here to verify the print messages if needed
⋮----
fn run_fails_if_path_does_not_exist() {
⋮----
let project_path = tmp_dir.path().join("non_existent_project");
⋮----
let err_str = result.unwrap_err().to_string();
assert!(err_str.contains("Failed to find project root") || err_str.contains("Failed to canonicalize"));
⋮----
fn run_fails_if_no_project_found() {
⋮----
let empty_dir = tmp_dir.path().join("empty_dir");
fs::create_dir(&empty_dir).unwrap();
let result = run(empty_dir.to_str().unwrap().to_string());
⋮----
let err = result.unwrap_err().to_string();
assert!(err.contains("project root"));
⋮----
fn run_fails_if_config_is_malformed() {
⋮----
// Overwrite with malformed config
fs::write(project_path.join("ao.toml"), "[project]name=").unwrap();
⋮----
assert!(err.contains("parse") && err.contains("config"));
⋮----
fn run_fails_if_structure_invalid_even_if_found() {
⋮----
// Test removing a required service directory
fs::remove_dir_all(project_path.join("api-service")).unwrap();
let result_dir = run(project_path.to_str().unwrap().to_string());
assert!(result_dir.is_err());
let err_msg_dir = result_dir.unwrap_err().to_string();
assert!(err_msg_dir.contains("Required directory") && err_msg_dir.contains("api-service"));
⋮----
// Recreate the project for the next check
let project_path = setup_valid_project(tmp_dir.path()).unwrap(); // Re-init
⋮----
// Test removing a required file within a service directory (proto)
fs::remove_file(project_path.join("model-interface/anops.proto")).unwrap();
let result_file_proto = run(project_path.to_str().unwrap().to_string());
assert!(result_file_proto.is_err());
let err_msg_proto = result_file_proto.unwrap_err().to_string();
assert!(err_msg_proto.contains("Required file") && err_msg_proto.contains("anops.proto") && err_msg_proto.contains("model-interface"));
⋮----
// Test removing a generated gRPC file
fs::remove_file(project_path.join("api-service/anops_pb2.py")).unwrap();
let result_file_grpc = run(project_path.to_str().unwrap().to_string());
assert!(result_file_grpc.is_err());
let err_msg_grpc = result_file_grpc.unwrap_err().to_string();
assert!(err_msg_grpc.contains("Required file") && err_msg_grpc.contains("anops_pb2.py") && err_msg_grpc.contains("api-service"));
</file>

<file path="ao-cli/src/config.rs">
use serde::Deserialize;
⋮----
use std::fs;
use std::path::Path;
use std::collections::HashMap; // Added HashMap
⋮----
/// Represents the overall configuration loaded from ao.toml
⋮----
pub struct Config {
⋮----
#[serde(default)] // Use default HashMap if missing
⋮----
/// Represents the [project] table in ao.toml
#[derive(Deserialize, Debug, PartialEq, Default)] // Added Default
pub struct ProjectConfig {
⋮----
/// Represents the [check] table in ao.toml
⋮----
pub struct CheckConfig {
⋮----
/// Loads the configuration from the ao.toml file in the project root.
///
/// # Arguments
⋮----
/// * `project_root` - The path to the project root directory (containing ao.toml).
⋮----
/// # Errors
⋮----
/// Returns an error if the config file cannot be read or parsed.
pub fn load_config(project_root: &Path) -> Result<Config> {
let config_path = project_root.join("ao.toml");
println!("Loading config from: {:?}", config_path);
⋮----
if !config_path.exists() {
⋮----
.with_context(|| format!("Failed to read config file: {}", config_path.display()))?;
⋮----
.with_context(|| format!("Failed to parse TOML config file: {}", config_path.display()))?;
⋮----
println!("Config loaded successfully: {:?}", config);
Ok(config)
⋮----
mod tests {
⋮----
use tempfile::tempdir;
⋮----
use std::path::PathBuf;
⋮----
// Helper to create a dummy ao.toml
fn create_dummy_config(dir: &Path, content: &str) -> PathBuf {
let config_path = dir.join("ao.toml");
fs::write(&config_path, content).unwrap();
⋮----
fn load_config_succeeds_with_valid_file_no_extras() {
let tmp_dir = tempdir().unwrap();
⋮----
let config_content = format!("[project]\nname = \"{}\"", project_name);
create_dummy_config(tmp_dir.path(), &config_content);
⋮----
let config = load_config(tmp_dir.path()).unwrap();
⋮----
assert_eq!(config.project.name, project_name);
assert_eq!(config.check, CheckConfig::default());
assert!(config.tasks.is_empty()); // Default tasks is empty map
⋮----
fn load_config_succeeds_with_tasks_section() {
⋮----
let config_content = format!("[project]\nname = \"{}\"\n\n[tasks]\nbuild = [\"echo building...\", \"mkdir dist\"]\ndeploy = [\"echo deploying...\"]", project_name);
⋮----
assert_eq!(config.tasks.len(), 2);
assert_eq!(config.tasks.get("build").unwrap(), &vec!["echo building...", "mkdir dist"]);
assert_eq!(config.tasks.get("deploy").unwrap(), &vec!["echo deploying..."]);
⋮----
fn load_config_succeeds_with_empty_tasks_section() {
⋮----
let config_content = format!("[project]\nname = \"{}\"\n\n[tasks] # Empty tasks table", project_name);
⋮----
assert!(config.tasks.is_empty());
⋮----
fn load_config_succeeds_with_all_sections() {
⋮----
let config_content = format!("[project]\nname = \"{}\"\n\n[check]\nlinters = [\"lint1\"]\n\n[tasks]\nbuild = [\"build1\"]", project_name);
⋮----
assert_eq!(config.check.linters, vec!["lint1"]);
assert!(config.check.testers.is_empty());
assert_eq!(config.tasks.len(), 1);
assert_eq!(config.tasks.get("build").unwrap(), &vec!["build1"]);
⋮----
fn load_config_fails_with_malformed_tasks() {
⋮----
let config_content = format!("[project]\nname = \"{}\"\n\n[tasks]\nbuild = \"not-an-array\"", project_name);
⋮----
let result = load_config(tmp_dir.path());
assert!(result.is_err());
let err = result.unwrap_err().to_string();
assert!(err.contains("invalid") && err.contains("type"));
⋮----
fn load_config_fails_with_malformed_task_steps() {
⋮----
let config_content = format!("[project]\nname = \"{}\"\n\n[tasks]\nbuild = [1, 2, 3] # Numbers instead of strings", project_name);
⋮----
fn load_config_fails_if_file_missing() {
⋮----
assert!(err.contains("not found"));
⋮----
fn load_config_fails_if_file_malformed() {
⋮----
create_dummy_config(tmp_dir.path(), malformed_content);
⋮----
assert!(err.contains("parse") || err.contains("expected") || err.contains("config"));
⋮----
fn load_config_fails_if_missing_project_table() {
⋮----
let content = "[tasks]\nbuild=['a']"; // Missing [project]
create_dummy_config(tmp_dir.path(), content);
⋮----
assert!(err.contains("missing") && err.contains("project"));
⋮----
fn load_config_fails_if_missing_project_name() {
⋮----
assert!(err.contains("missing") && err.contains("name"));
⋮----
fn load_config_handles_incorrect_types_in_check() {
⋮----
let config_content = format!("[project]\nname = \"{}\"\n\n[check]\nlinters = \"not-an-array\" # Incorrect type", project_name);
</file>

<file path="ao-cli/src/run.rs">
use crate::config;
⋮----
use std::env;
⋮----
use crate::utils::{find_project_root, run_tool}; // Import from utils
⋮----
// --- Helper Functions removed, now in utils.rs --- //
⋮----
// --- Main `run` function --- //
⋮----
/// Handler for `ao run <task_name>`.
/// Finds the project root, loads config, and executes the steps for the specified task.
///
/// # Arguments
⋮----
/// * `task_name` - The name of the task defined in `ao.toml` to execute.
/// * `path_str` - Path within the project directory to start searching from.
⋮----
/// # Errors
⋮----
/// Returns an error if the project root is not found, config loading fails,
/// the task is not found, or any command within the task fails.
pub fn run(task_name: String, path_str: String) -> Result<()> {
⋮----
info!("Running task '{}' starting from '{}'", task_name, start_path.display());
⋮----
// Find project root using the utility function
let project_path = find_project_root(start_path)
.with_context(|| format!("Failed to find project root starting from '{}'", start_path.display()))?;
info!("Found project root at '{}'", project_path.display());
⋮----
// Load configuration
⋮----
.context("Failed to load project configuration")?;
info!("Project name from config: {}", config.project.name);
⋮----
// Find the requested task
match config.tasks.get(&task_name) {
⋮----
info!("--- Running task '{}' ---", task_name);
if commands.is_empty() {
warn!("Task '{}' has no commands defined.", task_name);
⋮----
// Use the utility function to run the command
run_tool(command_str, &project_path).with_context(|| {
format!("Command '{}' in task '{}' failed", command_str, task_name)
⋮----
info!("--- Task '{}' finished successfully ---", task_name);
Ok(())
⋮----
error!("Task '{}' not found in ao.toml", task_name);
bail!("Task '{}' not found in ao.toml", task_name);
⋮----
mod tests {
⋮----
use crate::init; // To set up a project structure
use std::fs;
use tempfile::tempdir;
⋮----
// Helper to create a project with a specific ao.toml content
fn setup_project_with_config(base_path: &Path, config_content: &str) -> Result<PathBuf> {
let project_dir = base_path.join("test_run_project");
// Run init first to get base structure (it creates a basic ao.toml)
init::run(project_dir.to_str().unwrap().to_string())?;
// Overwrite ao.toml with specific content
let config_path = project_dir.join("ao.toml");
fs::write(config_path, config_content).context("Failed to write test config")?;
Ok(project_dir)
⋮----
fn run_succeeds_with_valid_task() {
let tmp_dir = tempdir().unwrap();
let project_name = "test_run_project"; // Name used inside config content
let config_content = format!(
⋮----
let project_path = setup_project_with_config(tmp_dir.path(), &config_content).unwrap();
⋮----
let result = run("build".to_string(), project_path.to_str().unwrap().to_string());
⋮----
assert!(result.is_ok());
// Check side effect of the command
assert!(project_path.join("build_output").exists());
assert!(project_path.join("build_output").is_dir());
⋮----
fn run_succeeds_with_empty_task() {
⋮----
let result = run("empty".to_string(), project_path.to_str().unwrap().to_string());
⋮----
// No side effects to check
⋮----
fn run_fails_if_task_not_found() {
⋮----
let result = run("deploy".to_string(), project_path.to_str().unwrap().to_string()); // Task 'deploy' doesn't exist
⋮----
assert!(result.is_err());
let err = result.unwrap_err().to_string();
assert!(err.contains("not found") && err.contains("deploy"));
⋮----
fn run_fails_if_command_in_task_fails() {
⋮----
// Use a command that will fail (ls on a non-existent file)
⋮----
let err_msg = result.unwrap_err().to_string();
// Loosened: check for 'failed' and 'ls' and 'build' somewhere in the error
assert!(err_msg.contains("failed") && err_msg.contains("ls") && err_msg.contains("build"));
⋮----
fn run_fails_if_project_root_not_found() {
⋮----
// Don't create any project or config
let non_project_path = tmp_dir.path().join("not_a_project");
fs::create_dir(&non_project_path).unwrap();
⋮----
let result = run(
"build".to_string(),
non_project_path.to_str().unwrap().to_string(),
⋮----
assert!(err.contains("project root"));
⋮----
fn run_fails_if_config_is_malformed() {
⋮----
// Create a project but with invalid TOML
let project_path = tmp_dir.path().join("malformed_config_project");
init::run(project_path.to_str().unwrap().to_string()).unwrap();
fs::write(project_path.join("ao.toml"), "[project]name=").unwrap(); // Malformed
⋮----
assert!(err.contains("parse") || err.contains("config") || err.contains("expected"));
⋮----
fn run_works_when_called_from_subdir() {
⋮----
let models_path = project_path.join("models"); // Subdir created by init
⋮----
// Run from the 'models' subdirectory
let result = run("build".to_string(), models_path.to_str().unwrap().to_string());
if result.is_err() {
eprintln!("run_works_when_called_from_subdir failed: {}", result.unwrap_err());
⋮----
// Check side effect relative to the project root
assert!(project_path.join("build_output_subdir").exists());
assert!(project_path.join("build_output_subdir").is_dir());
</file>

<file path="ao-cli/src/utils.rs">
use std::fs;
use shlex;
⋮----
/// Searches upwards from the starting path for a file named `ao.toml`.
/// Returns the path to the directory containing `ao.toml` if found.
pub fn find_project_root(start_path: &Path) -> Result<PathBuf> {
info!("find_project_root: Starting search from '{}'", start_path.display());
// Canonicalize the starting path to resolve symlinks and relative components
⋮----
.canonicalize()
.with_context(|| format!("Failed to canonicalize path: {}", start_path.display()))?;
info!("find_project_root: Canonical path is '{}'", current_path.display());
⋮----
let config_path = current_path.join("ao.toml");
info!("find_project_root: Checking for config at '{}'", config_path.display());
if config_path.exists() && config_path.is_file() {
info!("find_project_root: Found config at '{}'", config_path.display());
return Ok(current_path);
⋮----
// Move up to the parent directory
if let Some(parent) = current_path.parent() {
// Check if we are already at the root to prevent infinite loop
⋮----
warn!("find_project_root: Reached filesystem root, config not found.");
⋮----
info!("find_project_root: Moving up to parent '{}'", parent.display());
current_path = parent.to_path_buf();
⋮----
// Should not happen if parent == current_path check works, but as a safeguard
warn!("find_project_root: No parent found, config not found.");
⋮----
// If loop finishes without returning, the file was not found
bail!(
⋮----
/// Executes an external tool/command within the project directory.
///
/// # Arguments
⋮----
/// * `command_str` - The command string to execute (e.g., "ruff check .").
/// * `project_root` - The path to the project root directory, used as the working directory.
⋮----
/// # Errors
⋮----
/// Returns an error if the command cannot be executed or if it exits with a non-zero status.
pub fn run_tool(command_str: &str, project_root: &Path) -> Result<()> {
// Use shlex for robust shell-like parsing
⋮----
.ok_or_else(|| anyhow!("Failed to parse command string with shlex: '{}'", command_str))?;
if parts.is_empty() {
bail!("Command string '{}' resulted in no executable parts.", command_str);
⋮----
command.args(args);
command.current_dir(project_root);
command.stdout(Stdio::inherit());
command.stderr(Stdio::inherit());
⋮----
.status()
.with_context(|| format!("Failed to execute command: '{}'", command_str))?;
if status.success() {
info!("Tool '{}' finished successfully.", command_str);
return Ok(());
⋮----
command.stdout(Stdio::inherit()); // Stream stdout directly
command.stderr(Stdio::inherit()); // Stream stderr directly
⋮----
error!("Tool '{}' failed with status: {}", command_str, status);
bail!("Tool '{}' failed with status: {}", command_str, status);
⋮----
/// Generates gRPC code using python -m grpc_tools.protoc
/// Assumes proto files are in model-interface and outputs to api-service and model-service.
pub fn generate_grpc_code(project_root: &Path) -> Result<()> {
info!("--- Generating gRPC Code ---");
let interface_dir = project_root.join("model-interface");
let api_service_dir = project_root.join("api-service");
let model_service_dir = project_root.join("model-service");
let proto_file = interface_dir.join("anops.proto");
⋮----
if !proto_file.exists() {
bail!("Proto file not found at {}", proto_file.display());
⋮----
// Ensure output directories exist
⋮----
.with_context(|| format!("Failed to ensure api-service directory exists: {}", api_service_dir.display()))?;
⋮----
.with_context(|| format!("Failed to ensure model-service directory exists: {}", model_service_dir.display()))?;
⋮----
// Construct the command. Using Command directly to avoid run_tool's parsing issues for now.
// We run it from the project_root context.
// Note: Assumes 'python' and 'grpc_tools.protoc' are available in the PATH.
⋮----
command.arg("-m")
.arg("grpc_tools.protoc")
.arg(format!("-I{}", interface_dir.display())) // Include path for proto file
// Output to api-service
.arg(format!("--python_out={}", api_service_dir.display()))
.arg(format!("--pyi_out={}", api_service_dir.display()))
.arg(format!("--grpc_python_out={}", api_service_dir.display()))
// Output to model-service
.arg(format!("--python_out={}", model_service_dir.display()))
.arg(format!("--pyi_out={}", model_service_dir.display()))
.arg(format!("--grpc_python_out={}", model_service_dir.display()))
// The proto file itself (relative to include path)
.arg(proto_file.file_name().unwrap().to_str().unwrap()); // Use just the filename relative to -I
⋮----
command.current_dir(project_root); // Run from project root
⋮----
info!("Executing: {:?}", command);
⋮----
.context("Failed to execute python -m grpc_tools.protoc command. Is grpcio-tools installed and python in PATH?")?;
⋮----
info!("gRPC code generated successfully.");
info!("--- gRPC Code Generation Finished ---");
Ok(())
⋮----
error!("gRPC code generation failed with status: {}", status);
bail!("gRPC code generation failed with status: {}", status);
⋮----
mod tests {
⋮----
use crate::init; // To setup project structure
⋮----
use tempfile::tempdir;
⋮----
// TODO: Add tests for find_project_root
// TODO: Add tests for run_tool (mocking Command)
⋮----
fn generate_grpc_code_runs_without_panic_on_valid_structure() {
// This is a basic test to ensure the function can be called,
// finds paths, and attempts to run the command without panicking.
// It DOES NOT verify the command executes correctly or files are generated,
// as that requires python/grpcio-tools and filesystem changes.
// TODO: Implement proper mocking of std::process::Command for robust testing.
⋮----
let tmp_dir = tempdir().unwrap();
⋮----
let project_path = tmp_dir.path().join(project_name);
⋮----
// Use init::run to create the necessary structure
init::run(project_path.to_str().unwrap().to_string()).unwrap();
⋮----
// Ensure the proto file exists (created by init::run)
assert!(project_path.join("model-interface/anops.proto").exists());
⋮----
// Call the function - we expect Ok(()) if it constructs the command,
// even if the command itself fails externally.
// If python/grpcio-tools are not installed, this might return Err,
// but the test aims to catch panics within generate_grpc_code itself.
let result = generate_grpc_code(&project_path);
⋮----
// Basic assertion: Check if the function completed its logic.
// If python/grpcio-tools aren't installed, it will likely return Err here.
// If they ARE installed, it should return Ok.
// We accept either Ok or an Err containing the execution failure message.
⋮----
Ok(_) => info!("generate_grpc_code returned Ok (python/grpcio-tools likely found)"),
⋮----
let msg = e.to_string();
warn!("generate_grpc_code returned Err: {} (python/grpcio-tools likely not found or failed)", msg);
// Check it's the expected execution error, not a setup error
assert!(msg.contains("Failed to execute") || msg.contains("gRPC code generation failed"));
⋮----
fn generate_grpc_code_fails_if_proto_missing() {
⋮----
// Create partial structure WITHOUT the proto file
fs::create_dir_all(project_path.join("model-interface")).unwrap();
fs::create_dir_all(project_path.join("api-service")).unwrap();
fs::create_dir_all(project_path.join("model-service")).unwrap();
// Create ao.toml so find_project_root works if called implicitly later
fs::write(project_path.join("ao.toml"), "[project]\nname=\"test\"").unwrap();
⋮----
assert!(result.is_err());
assert!(result.unwrap_err().to_string().contains("Proto file not found"));
⋮----
fn run_tool_succeeds_with_echo() {
use std::env;
⋮----
// Use a harmless command that works on all platforms
let result = run_tool("echo hello", tmp_dir.path());
assert!(result.is_ok());
⋮----
// Note: For more robust mocking of external commands, consider using the 'assert_cmd' crate or similar in the future.
</file>

<file path="ao-cli/Cargo.toml">
[package]
name = "ao"
version = "0.1.0"
edition = "2021"

[dependencies]
clap = { version = "4.2", features = ["derive"] }
anyhow = "1.0"
toml = "0.8"
serde = { version = "1.0", features = ["derive"] }
log = "0.4"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["fmt", "json", "env-filter"] }
shlex = "1.1"

[dev-dependencies]
tempfile = "3.10"
</file>

<file path="api-service/main.py">
# Ensure log directory exists
⋮----
log_file = "/app/logs/api-service.log"
⋮----
logHandler = logging.FileHandler(log_file)
formatter = jsonlogger.JsonFormatter()
⋮----
format=None,  # Formatter is set above
⋮----
logger = logging.getLogger(__name__)
⋮----
# Import the generated gRPC classes
# Assumes the generated files (_pb2.py, _pb2_grpc.py) are accessible.
# This might require adjusting PYTHONPATH or copying files during build.
# For simplicity, we'll assume they are in the same directory or PYTHONPATH.
⋮----
app = FastAPI()
⋮----
# Get the model service URL from environment variable
MODEL_SERVICE_URL = os.getenv("MODEL_SERVICE_URL", "localhost:50051")
⋮----
# Pydantic model for request body
class PredictRequestData(BaseModel)
⋮----
input_data: str
⋮----
# Pydantic model for response body
class PredictResponseData(BaseModel)
⋮----
output_data: str
⋮----
@app.post("/predict", response_model=PredictResponseData)
async def predict(request_data: PredictRequestData)
⋮----
# Establish insecure gRPC channel to the model service
# In production, use secure channels (grpc.secure_channel)
⋮----
stub = anops_pb2_grpc.AnOpsStub(channel)
⋮----
# Create the gRPC request message
grpc_request = anops_pb2.PredictRequest(input_data=request_data.input_data)
⋮----
# Make the asynchronous gRPC call
grpc_response = await stub.Predict(grpc_request)
⋮----
# Return the response data
⋮----
@app.get("/health")
async def health_check()
⋮----
# Basic health check endpoint
⋮----
# This block is mainly for local development testing if needed,
# Uvicorn will run the app in the Docker container.
</file>

<file path="api-service/requirements.txt">
fastapi>=0.95.0,<1.0.0
uvicorn[standard]>=0.20.0,<1.0.0
grpcio>=1.50.0,<2.0.0
python-json-logger>=2.0.7,<3.0.0
# grpcio-tools is needed only for code generation, not runtime
</file>

<file path="model-service/requirements.txt">
grpcio>=1.50.0,<2.0.0
grpcio-tools>=1.50.0,<2.0.0
python-json-logger>=2.0.7,<3.0.0
# Add other model-specific dependencies here (e.g., scikit-learn, pandas)
</file>

<file path="model-service/server.py">
import os  # Added os
⋮----
# Ensure log directory exists
⋮----
log_file = "/app/logs/model-service.log"
logHandler = logging.FileHandler(log_file)
formatter = jsonlogger.JsonFormatter()
⋮----
logger = logging.getLogger(__name__)
⋮----
# Import the generated classes
⋮----
# --- Simple Model Logic --- #
def load_model_resource()
⋮----
"""Placeholder for loading any model artifacts (files, connections, etc.)."""
# Example: Load a prefix from an environment variable or a file
prefix = os.getenv("MODEL_PREFIX", "MODEL_OUTPUT:")
⋮----
# Load resource once when the server starts (or lazily on first request)
MODEL_PREFIX = load_model_resource()
⋮----
def run_model(input_str: str) -> str
⋮----
"""Placeholder for actual model execution."""
⋮----
# Example: Add a prefix and convert to uppercase
output_str = f"{MODEL_PREFIX} {input_str.upper()}"
⋮----
# ------------------------ #
⋮----
# Implementation of the AnOps service
class AnOpsServicer(anops_pb2_grpc.AnOpsServicer)
⋮----
"""Provides methods that implement functionality of the AnOps server."""
⋮----
def Predict(self, request, context)
⋮----
"""Handles the Predict RPC call."""
⋮----
# --- Call the model logic --- #
⋮----
output = run_model(request.input_data)
⋮----
return anops_pb2.PredictResponse()  # Return empty response on error
⋮----
logger.error(f"Model execution failed: {e}", exc_info=True)  # Log traceback
⋮----
# -------------------------- #
⋮----
# Function to start the server
def serve()
⋮----
server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
⋮----
port = "[::]:50051"  # Listen on all interfaces, port 50051
⋮----
# Keep the server running
⋮----
time.sleep(86400)  # Keep alive for one day
⋮----
server.stop(0)  # Graceful stop with no grace period
⋮----
# TODO: Add unit tests for the model logic (run_model, load_model_resource).
# TODO: Add integration tests for the gRPC server (AnOpsServicer.Predict).
</file>

<file path="ao-cli/src/init.rs">
use std::fs;
use std::path::Path;
⋮----
// Basic placeholder content for generated files
⋮----
/// Handler for `ao init`.
/// Creates the basic project directory structure and configuration file.
///
/// # Arguments
⋮----
/// * `name` - The name of the project directory to initialize.
⋮----
/// # Errors
⋮----
/// Returns an error if initialization fails (e.g., directory creation, file creation).
pub fn run(name: String) -> Result<()> {
⋮----
info!("Initializing project '{}' at {:?}", name, project_path);
⋮----
// Create base directory
⋮----
.with_context(|| format!("Failed to create project directory: {}", project_path.display()))?;
info!("Created directory: {:?}", project_path);
⋮----
// Create standard subdirectories
⋮----
for subdir in subdirs.iter() {
let dir_path = project_path.join(subdir);
⋮----
.with_context(|| format!("Failed to create subdirectory: {}", dir_path.display()))?;
info!("Created directory: {:?}", dir_path);
⋮----
// Create ao.toml configuration file
let config_path = project_path.join("ao.toml");
let config_content = DEFAULT_AO_TOML_CONTENT.replace("{}", &name);
⋮----
.with_context(|| format!("Failed to write config file: {}", config_path.display()))?;
info!("Created config file: {:?}", config_path);
⋮----
// Create .gitignore
let gitignore_path = project_path.join(".gitignore");
⋮----
.with_context(|| format!("Failed to write .gitignore file: {}", gitignore_path.display()))?;
info!("Created file: {:?}", gitignore_path);
⋮----
// Create Dockerfiles
let api_dockerfile_path = project_path.join("api-service/Dockerfile");
⋮----
.with_context(|| format!("Failed to write api-service Dockerfile: {}", api_dockerfile_path.display()))?;
info!("Created file: {:?}", api_dockerfile_path);
⋮----
let model_dockerfile_path = project_path.join("model-service/Dockerfile");
⋮----
.with_context(|| format!("Failed to write model-service Dockerfile: {}", model_dockerfile_path.display()))?;
info!("Created file: {:?}", model_dockerfile_path);
⋮----
// Create docker-compose.yml
let compose_path = project_path.join("docker-compose.yml");
⋮----
.with_context(|| format!("Failed to write docker-compose.yml: {}", compose_path.display()))?;
info!("Created file: {:?}", compose_path);
⋮----
// Create model-interface proto file
let proto_path = project_path.join("model-interface/anops.proto");
⋮----
.with_context(|| format!("Failed to write anops.proto: {}", proto_path.display()))?;
info!("Created file: {:?}", proto_path);
⋮----
// Create READMEs
let api_readme_path = project_path.join("api-service/README.md");
⋮----
.with_context(|| format!("Failed to write api-service README: {}", api_readme_path.display()))?;
info!("Created file: {:?}", api_readme_path);
⋮----
let model_readme_path = project_path.join("model-service/README.md");
⋮----
.with_context(|| format!("Failed to write model-service README: {}", model_readme_path.display()))?;
info!("Created file: {:?}", model_readme_path);
⋮----
let interface_readme_path = project_path.join("model-interface/README.md");
⋮----
.with_context(|| format!("Failed to write model-interface README: {}", interface_readme_path.display()))?;
info!("Created file: {:?}", interface_readme_path);
⋮----
// Create placeholder files in services (optional, but good practice)
// e.g., api-service/main.py, model-service/server.py
// fs::write(project_path.join("api-service/main.py"), "# FastAPI app placeholder")?;
// fs::write(project_path.join("model-service/server.py"), "# gRPC server placeholder")?;
⋮----
info!("Project '{}' initialized successfully.", name);
info!("Next steps for '{}':", name);
info!("  - cd {}", name);
info!("  - Review READMEs in api-service, model-service, model-interface.");
info!("  - Implement your model in model-service.");
info!("  - Implement the API endpoints in api-service.");
info!("  - Generate gRPC code (see model-interface/README.md).");
info!("  - Configure dependencies (e.g., requirements.txt).");
info!("  - Run 'ao build' to build the service images.");
info!("  - Run 'docker-compose up' to start the services.");
⋮----
Ok(())
⋮----
mod tests {
⋮----
use tempfile::tempdir;
⋮----
fn run_succeeds_and_creates_structure() {
let tmp_dir = tempdir().unwrap();
⋮----
// Run the init command relative to the temp dir
let result = run(tmp_dir.path().join(project_name).to_str().unwrap().to_string());
assert!(result.is_ok());
⋮----
let project_path = tmp_dir.path().join(project_name);
⋮----
// Check if base directory exists
assert!(project_path.exists());
assert!(project_path.is_dir());
⋮----
// Check if standard subdirectories exist
⋮----
assert!(dir_path.exists(), "Directory missing: {}", subdir);
assert!(dir_path.is_dir(), "Path is not a directory: {}", subdir);
⋮----
// Check if core files exist
⋮----
for file in core_files.iter() {
let file_path = project_path.join(file);
assert!(file_path.exists(), "File missing: {}", file);
assert!(file_path.is_file(), "Path is not a file: {}", file);
⋮----
// Check if config file has basic content (loosened: just check for [project] and project_name)
⋮----
let content = fs::read_to_string(config_path).unwrap();
assert!(content.contains("[project]"));
assert!(content.contains(project_name));
⋮----
// Check .gitignore content (basic check)
⋮----
let gitignore_content = fs::read_to_string(gitignore_path).unwrap();
assert!(gitignore_content.contains("__pycache__/"));
assert!(gitignore_content.contains("*.pyc"));
// Clean up is handled by tempdir dropping
⋮----
fn run_fails_if_cannot_create_dir() {
// This test is tricky because permissions are hard to simulate reliably
// across platforms in a unit test without running as root or modifying
// system state. We rely on the OS preventing creation in restricted areas.
// Trying to create directly in `/` (on Unix-like systems) might fail
// without root privileges.
if cfg!(unix) {
⋮----
// Attempt to run the init command in a restricted path
let result = run(project_name.to_string());
// We expect this to fail, likely with a permission error context.
assert!(result.is_err());
assert!(result.unwrap_err().to_string().contains("Failed to create project directory"));
⋮----
// Skip this specific scenario on non-Unix platforms where
// root directory permissions might behave differently.
println!("Skipping root directory creation test on non-Unix platform.");
</file>

</files>
